---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

## Question 1: Bivariate Regression with Stress Survey

### Regression of Anxiety on Stress Survey

```{python}
#| echo: false
# Fit the bivariate regression model
X1 = observDF[['StressSurvey']]
y = observDF['Anxiety']

# Using statsmodels for detailed output
X1_sm = sm.add_constant(X1)
model1 = sm.OLS(y, X1_sm).fit()

# Display the regression results
print(model1.summary())
```


### Estimated Coefficients

```{python}
#| echo: false
# Print the estimated coefficients
intercept = model1.params['const']
stress_coef = model1.params['StressSurvey']

print("Estimated Coefficients:")
print(f"Intercept (β₀): {intercept:.4f}")
print(f"StressSurvey (β₁): {stress_coef:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept:.4f} + {stress_coef:.4f} × StressSurvey")
```

### Interpretation

---

**INTERPRETATION:**

The regression using StressSurvey produces the equation Anxiety = –1.52 + 1.05 × StressSurvey. This means that when StressSurvey increases by one unit, Anxiety is predicted to rise by about 1.05 units. The intercept of –1.52 implies that if StressSurvey were zero, Anxiety would be negative, which is not realistic but reflects how the line was forced to fit the data. Compared to the true relationship, which is Anxiety = Stress + 0.1 × Time, the slope is too high. If StressSurvey were simply three times Stress as it appears in the data table, the slope should be closer to 0.33, not 1.05. The reason for the inflation is that the model leaves out Time, which is correlated with StressSurvey, so the regression mistakenly shifts part of Time's effect onto StressSurvey. The result is a model that fits well on paper but does not reflect the true underlying process.

---

## Question 2: Visualization of Bivariate Relationship

### Scatter Plot Analysis and Commentary

```{python}
#| echo: false
import matplotlib.pyplot as plt
from scipy import stats

# Create a detailed scatter plot with additional analysis
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Original scatter plot with regression line
ax1.scatter(observDF['StressSurvey'], observDF['Anxiety'], color='blue', s=80, alpha=0.7)
ax1.plot(observDF['StressSurvey'], model1.fittedvalues, color='red', linewidth=2, label='Regression Line')

# Add confidence interval
newx = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
newx_df = pd.DataFrame({'const': 1, 'StressSurvey': newx})
pred = model1.get_prediction(newx_df)
pred_summary = pred.summary_frame(alpha=0.05)
ax1.plot(newx, pred_summary['mean_ci_lower'], 'r--', linewidth=1, label='95% CI')
ax1.plot(newx, pred_summary['mean_ci_upper'], 'r--', linewidth=1)
ax1.set_xlabel('Stress Survey')
ax1.set_ylabel('Anxiety')
ax1.set_title('Anxiety vs Stress Survey\nwith Regression Line')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Residuals plot to check assumptions
residuals = model1.resid
ax2.scatter(observDF['StressSurvey'], residuals, color='darkgreen', s=80, alpha=0.7)
ax2.axhline(y=0, color='red', linewidth=2)
ax2.set_xlabel('Stress Survey')
ax2.set_ylabel('Residuals')
ax2.set_title('Residuals vs Stress Survey\n(Checking Linearity)')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Save the main scatter plot
fig2, ax = plt.subplots(figsize=(8, 6))
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], color='blue', s=80, alpha=0.7)
ax.plot(observDF['StressSurvey'], model1.fittedvalues, color='red', linewidth=2)
ax.plot(newx, pred_summary['mean_ci_lower'], 'r--', linewidth=1)
ax.plot(newx, pred_summary['mean_ci_upper'], 'r--', linewidth=1)
ax.set_xlabel('Stress Survey')
ax.set_ylabel('Anxiety')
ax.set_title('Bivariate Regression: Anxiety vs Stress Survey')
ax.grid(True, alpha=0.3)
plt.savefig('anxiety_stress_regression.png', dpi=100, bbox_inches='tight')
plt.close()
```

### Scatter Plot Interpretation

---

**INTERPRETATION:**

The scatter plot shows a strong linear relationship between StressSurvey and Anxiety with a high R^2 value. Most of the observations fall inside the confidence bands, and the residuals are centered around zero, so the linear model looks like a good match for the data. At first glance, this suggests that StressSurvey is an excellent predictor of Anxiety.

However, upon further examination there are some potential issues. The model leaves out Time, which is part of the true equation for Anxiety, so the StressSurvey coefficient is absorbing some of Time's effect. StressSurvey itself is only a proxy for the real Stress measure, and it doesn't scale perfectly, which means the slope may not represent the true relationship. Even though the R² value is very high, that alone doesn't prove the coefficients are correct.

---

## Question 3: Bivariate Regression of Anxiety on Time

### Regression of Anxiety on Time

```{python}
#| echo: false
# Fit the bivariate regression model: Anxiety ~ Time
X2 = observDF[['Time']]
X2_sm = sm.add_constant(X2)
model2 = sm.OLS(y, X2_sm).fit()

# Display the regression results
print(model2.summary())
```

### Estimated Coefficients for Time Model

```{python}
#| echo: false
# Print the estimated coefficients for the Time model
intercept_time = model2.params['const']
time_coef = model2.params['Time']

print("Estimated Coefficients (Anxiety ~ Time):")
print("======================================")
print(f"Intercept (β₀): {intercept_time:.4f}")
print(f"Time (β₁): {time_coef:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_time:.4f} + {time_coef:.4f} × Time")
```

### Interpretation

---

**INTERPRETATION:**

The regression using Time produces the equation Anxiety = –3.68 + 5.34 × Time. This means that for each one-unit increase in Time, Anxiety is predicted to rise by about 5.34 units. The intercept of –3.68 suggests that if Time were zero, Anxiety would be negative, which isn't realistic but reflects how the line was adjusted to fit the data. Compared to the true relationship, which is Anxiety = Stress + 0.1 × Time, the slope is far too large. The true effect of Time is only 0.1, but because Stress and Time rise together in this dataset, the regression gives Time credit for Stress's much stronger effect. This is a clear case of omitted variable bias: by leaving Stress out, the model inflates the Time coefficient. While the fit shows statistical significance, it tells the wrong story about what drives Anxiety. In practice, the model looks convincing but is misleading because it exaggerates Time's role and ignores the real driver.

---

## Question 4: Visualization of Bivariate Relationship

### Scatter Plot: Anxiety vs Time

```{python}
#| echo: false
# Create scatter plot for Anxiety vs Time
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(observDF['Time'], observDF['Anxiety'], color='purple', s=80, alpha=0.7)
ax.plot(observDF['Time'], model2.fittedvalues, color='red', linewidth=2, label='Regression Line')

# Add confidence interval
newx_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
newx_time_df = pd.DataFrame({'const': 1, 'Time': newx_time})
pred_time = model2.get_prediction(newx_time_df)
pred_time_summary = pred_time.summary_frame(alpha=0.05)
ax.plot(newx_time, pred_time_summary['mean_ci_lower'], 'r--', linewidth=1, label='95% CI')
ax.plot(newx_time, pred_time_summary['mean_ci_upper'], 'r--', linewidth=1)
ax.set_xlabel('Time')
ax.set_ylabel('Anxiety')
ax.set_title('Anxiety vs Time\nwith Regression Line')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Save the plot
fig2, ax2 = plt.subplots(figsize=(8, 6))
ax2.scatter(observDF['Time'], observDF['Anxiety'], color='purple', s=80, alpha=0.7)
ax2.plot(observDF['Time'], model2.fittedvalues, color='red', linewidth=2)
ax2.plot(newx_time, pred_time_summary['mean_ci_lower'], 'r--', linewidth=1)
ax2.plot(newx_time, pred_time_summary['mean_ci_upper'], 'r--', linewidth=1)
ax2.set_xlabel('Time')
ax2.set_ylabel('Anxiety')
ax2.set_title('Bivariate Regression: Anxiety vs Time')
ax2.grid(True, alpha=0.3)
plt.savefig('anxiety_time_regression.png', dpi=100, bbox_inches='tight')
plt.close()
```

### Scatter Plot Interpretation

---

**INTERPRETATION:**

The scatter plot shows a strong linear relationship between Time and Anxiety with a high R^2 value. Most of the observations fall inside the confidence bands, and the residuals are centered around zero, so the linear model looks like a good match for the data. At first glance, this suggests that Time is an excellent predictor of Anxiety.

However, upon further examination there are some potential issues. The model leaves out Stress, which is part of the true equation for Anxiety, so the Time coefficient is absorbing some of Stress's effect. Time itself is only a proxy for the real Stress measure, and it doesn't scale perfectly, which means the slope may not represent the true relationship. Even though the R² value is very high, that alone doesn't prove the coefficients are correct. Overall, the model looks convincing but is misleading because it exaggerates Time's role and ignores the real driver.

---

## Question 5: Multiple Regression of Anxiety on StressSurvey and Time


### Multiple Regression Model

```{python}
#| echo: false
# Fit the multiple regression model: Anxiety ~ StressSurvey + Time
X3 = observDF[['StressSurvey', 'Time']]
X3_sm = sm.add_constant(X3)
model3 = sm.OLS(y, X3_sm).fit()

# Display the regression results
print(model3.summary())
```

### Estimated Coefficients for Multiple Regression

```{python}
#| echo: false
# Print the estimated coefficients for the multiple regression model
intercept_mult = model3.params['const']
stress_coef_mult = model3.params['StressSurvey']
time_coef_mult = model3.params['Time']

print("Estimated Coefficients (Anxiety ~ StressSurvey + Time):")
print("=====================================================")
print(f"Intercept (β₀): {intercept_mult:.4f}")
print(f"StressSurvey (β₁): {stress_coef_mult:.4f}")
print(f"Time (β₂): {time_coef_mult:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_mult:.4f} + {stress_coef_mult:.4f} × StressSurvey + {time_coef_mult:.4f} × Time")
```

### Scatter Plot: Multiple Regression Visualization

```{python}
#| echo: false
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Get R-squared for the plot title
r_squared_multiple = model3.rsquared

# Create a comprehensive visualization with multiple subplots
fig = plt.figure(figsize=(16, 5))

# Plot 1: 3D Scatter Plot with Regression Plane
ax1 = fig.add_subplot(131, projection='3d')

# Scatter plot of actual data
scatter = ax1.scatter(observDF['StressSurvey'], observDF['Time'], observDF['Anxiety'], 
                     c='blue', marker='o', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Create a mesh for the regression plane
stress_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 20)
time_range = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 20)
stress_mesh, time_mesh = np.meshgrid(stress_range, time_range)

# Calculate predicted values for the mesh
anxiety_mesh = intercept_mult + stress_coef_mult * stress_mesh + time_coef_mult * time_mesh

# Plot the regression plane
surf = ax1.plot_surface(stress_mesh, time_mesh, anxiety_mesh, alpha=0.3, color='red')

ax1.set_xlabel('Stress Survey', fontsize=10)
ax1.set_ylabel('Time', fontsize=10)
ax1.set_zlabel('Anxiety', fontsize=10)
ax1.set_title('3D View: Multiple Regression\nAnxiety ~ StressSurvey + Time', fontsize=11, fontweight='bold')
ax1.view_init(elev=20, azim=45)

# Plot 2: Actual vs Fitted Values
ax2 = fig.add_subplot(132)

fitted_values_mult = model3.fittedvalues
ax2.scatter(observDF['Anxiety'], fitted_values_mult, color='darkblue', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Add perfect prediction line
min_val = min(observDF['Anxiety'].min(), fitted_values_mult.min())
max_val = max(observDF['Anxiety'].max(), fitted_values_mult.max())
ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')

ax2.set_xlabel('Actual Anxiety', fontsize=10)
ax2.set_ylabel('Fitted Anxiety', fontsize=10)
ax2.set_title('Actual vs Fitted Values\n(R² = {:.4f})'.format(r_squared_multiple), fontsize=11, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Residuals Plot
ax3 = fig.add_subplot(133)

residuals_mult = model3.resid
ax3.scatter(fitted_values_mult, residuals_mult, color='darkgreen', s=100, alpha=0.7, edgecolors='black', linewidth=1)
ax3.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax3.set_xlabel('Fitted Values', fontsize=10)
ax3.set_ylabel('Residuals', fontsize=10)
ax3.set_title('Residuals vs Fitted Values\n(Checking Assumptions)', fontsize=11, fontweight='bold')
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('multiple_regression_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n✓ Multiple regression visualization saved as 'multiple_regression_visualization.png'")
```

### Interpretation: Comparison to True Relationship

---

**INTERPRETATION:**

The multiple regression gives the equation Anxiety = 0.589 + 1.427 × StressSurvey − 2.780 × Time, with R² = 0.935. This suggests that higher StressSurvey scores increase Anxiety, while more Time lowers Anxiety. The negative sign on Time is unexpected since the true effect should be slightly positive. The issue is that StressSurvey and Time are closely related in this dataset, so the model has trouble separating their impacts. This multicollinearity makes StressSurvey’s effect look too strong and flips Time in the wrong direction. StressSurvey is also not a perfect stand-in for Stress, which adds more error. Even though the R² is high, the coefficients are misleading, so the model looks convincing but does not reflect the true relationship.

---


## Multiple Regression of Anxiety on Stress and Time


### Multiple Regression Model (Using Actual Stress)

```{python}
#| echo: false
# Fit the multiple regression model: Anxiety ~ Stress + Time
X4 = observDF[['Stress', 'Time']]
X4_sm = sm.add_constant(X4)
model4 = sm.OLS(y, X4_sm).fit()

# Display the regression results
print(model4.summary())
```

### Estimated Coefficients (Stress + Time Model)

```{python}
#| echo: false
# Print the estimated coefficients for Stress + Time model
intercept_stress = model4.params['const']
stress_coef_true = model4.params['Stress']
time_coef_stress = model4.params['Time']

print("Estimated Coefficients (Anxiety ~ Stress + Time):")
print("================================================")
print(f"Intercept (β₀): {intercept_stress:.4f}")
print(f"Stress (β₁): {stress_coef_true:.4f}")
print(f"Time (β₂): {time_coef_stress:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_stress:.4f} + {stress_coef_true:.4f} × Stress + {time_coef_stress:.4f} × Time")
```

### Scatter Plot: Multiple Regression Visualization (Stress + Time Model)

```{python}
#| echo: false
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Get R-squared for the plot title
r_squared_stress_time = model4.rsquared

# Create a comprehensive visualization with multiple subplots
fig = plt.figure(figsize=(16, 5))

# Plot 1: 3D Scatter Plot with Regression Plane
ax1 = fig.add_subplot(131, projection='3d')

# Scatter plot of actual data
scatter = ax1.scatter(observDF['Stress'], observDF['Time'], observDF['Anxiety'], 
                     c='darkgreen', marker='o', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Create a mesh for the regression plane
stress_range = np.linspace(observDF['Stress'].min(), observDF['Stress'].max(), 20)
time_range = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 20)
stress_mesh, time_mesh = np.meshgrid(stress_range, time_range)

# Calculate predicted values for the mesh
anxiety_mesh = intercept_stress + stress_coef_true * stress_mesh + time_coef_stress * time_mesh

# Plot the regression plane
surf = ax1.plot_surface(stress_mesh, time_mesh, anxiety_mesh, alpha=0.3, color='red')

ax1.set_xlabel('Stress', fontsize=10)
ax1.set_ylabel('Time', fontsize=10)
ax1.set_zlabel('Anxiety', fontsize=10)
ax1.set_title('3D View: Multiple Regression\nAnxiety ~ Stress + Time', fontsize=11, fontweight='bold')
ax1.view_init(elev=20, azim=45)

# Plot 2: Actual vs Fitted Values
ax2 = fig.add_subplot(132)

fitted_values_stress = model4.fittedvalues
ax2.scatter(observDF['Anxiety'], fitted_values_stress, color='darkgreen', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Add perfect prediction line
min_val = min(observDF['Anxiety'].min(), fitted_values_stress.min())
max_val = max(observDF['Anxiety'].max(), fitted_values_stress.max())
ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')

ax2.set_xlabel('Actual Anxiety', fontsize=10)
ax2.set_ylabel('Fitted Anxiety', fontsize=10)
ax2.set_title('Actual vs Fitted Values\n(R² = {:.4f})'.format(r_squared_stress_time), fontsize=11, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Residuals Plot
ax3 = fig.add_subplot(133)

residuals_stress = model4.resid
ax3.scatter(fitted_values_stress, residuals_stress, color='purple', s=100, alpha=0.7, edgecolors='black', linewidth=1)
ax3.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax3.set_xlabel('Fitted Values', fontsize=10)
ax3.set_ylabel('Residuals', fontsize=10)
ax3.set_title('Residuals vs Fitted Values\n(Checking Assumptions)', fontsize=11, fontweight='bold')
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('stress_time_multiple_regression.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n✓ Stress + Time multiple regression visualization saved as 'stress_time_multiple_regression.png'")
```


## Question 6: Model Comparison

### Comparison with True Relationship (Perfect Match Expected!)

---

**INTERPRETATION:**

The multiple regression of Anxiety on both Stress and Time gives us Anxiety = 0.000 + 1.000 × Stress + 0.100 × Time, with R² = 1.00. This happens to be the True coefficients for the relationship.. This shows that with the right varaibles, multiple regression can recover the true relationship. However, the difficulty comes when we don't know the true variables, and we have to use proxy variables.

---

## Question 7: Comparing the Two Multiple Regression Models

### Model Comparison Analysis

---

**INTERPRETATION:**

Both models show really high R-squared values (.935 and 1.00), which means they explain almost all of the changes in Anxiety. But that doesn't mean the models are actually good. In the model with StressSurvey and Time, the coefficients don't make much sense since StressSurvey comes out way too big and Time is negative even though it should be positive. The p-values show that the results are statistically significant, but that only means there is a strong pattern in the data, not that the coefficients are actually correct. This problem happens because StressSurvey is only a proxy and also because StressSurvey and Time move together, which messes up the estimates.

The model with Stress and Time works better because the coefficients match the true equation, with Stress equal to 1.00 and Time equal to 0.10. These are also statistically significant and actually meaningful.

Overall, this shows that even if a regression has a really high R-squared and very small p-values, that doesn't mean the model is right. If the wrong variables are used or if they overlap too much, the results can still be misleading. In real-world situations, it is important to use the right variables and not just look at the statistics when making decisions.

---

## Question 8: Reflect on Real-World Implications

### Contrasting Headlines

---

**INTERPRETATION:**

**For the multiple regression of Anxiety on both StressSurvey and Time**, we could expect to see a headline such as:

> ### **"Time on Social Media Found to DECREASE Anxiety"**

**For the multiple regression of Anxiety on Stress and Time**, we could expect to see a headline such as:

> ### **"Time on Social Media Found to INCREASE Anxiety"**

**Who Believes Which Headline?**

**A Typical Parent** will likely believe the second headline, as they likely see the negative effects of time on social media on their children. Additionally, the parent will likely be looking for reasons to reduce the time their children spend on social media.

**A Social Media Executive** will likely prefer the first headline, as they can use it to argue that time on social media is a good thing, and that it leads to decreased anxiety.

---

## Question 9: Avoiding Misleading Statistical Significance

```{python}
#| echo: false
# Create two subsets based on StressSurvey values
subset_low = observDF[observDF['StressSurvey'] < 6]
subset_high = observDF[observDF['StressSurvey'] >= 6]
```

### Subsetting Data and Analyzing Patterns

---

**INTERPRETATION:**

I split the data into low-stress and high-stress subsets using StressSurvey < 6 and StressSurvey ≥ 6. Below 6, StressSurvey scales neatly at about three times Stress, so it works as a reliable proxy and the regression should capture the true relationship. Above 6, the scaling breaks down as StressSurvey flattens and no longer rises in step with Stress, which is where the regression becomes distorted. This split made it possible to see clearly where the proxy performs well and where it causes problems.

---

### Graphical Diagnostics for Linearity

```{python}
#| echo: false
import matplotlib.pyplot as plt

# First, fit the models for both subsets
X_low = subset_low[['StressSurvey', 'Time']]
y_low = subset_low['Anxiety']
X_low_sm = sm.add_constant(X_low)
model_low = sm.OLS(y_low, X_low_sm).fit()

X_high = subset_high[['StressSurvey', 'Time']]
y_high = subset_high['Anxiety']
X_high_sm = sm.add_constant(X_high)
model_high = sm.OLS(y_high, X_high_sm).fit()

# Create comprehensive diagnostics for both subsets
fig = plt.figure(figsize=(16, 10))

# ===== LOW STRESS SUBSET DIAGNOSTICS =====
# Plot 1: Residuals vs Fitted (Low Stress)
ax1 = fig.add_subplot(2, 3, 1)
fitted_low = model_low.fittedvalues
residuals_low = model_low.resid
ax1.scatter(fitted_low, residuals_low, color='blue', s=80, alpha=0.7, edgecolors='black', linewidth=1)
ax1.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax1.set_xlabel('Fitted Values', fontsize=10)
ax1.set_ylabel('Residuals', fontsize=10)
ax1.set_title('Low Stress: Residuals vs Fitted\n(Checking Linearity)', fontsize=11, fontweight='bold')
ax1.grid(True, alpha=0.3)

# Plot 2: StressSurvey vs Residuals (Low Stress)
ax2 = fig.add_subplot(2, 3, 2)
ax2.scatter(subset_low['StressSurvey'], residuals_low, color='blue', s=80, alpha=0.7, edgecolors='black', linewidth=1)
ax2.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax2.set_xlabel('StressSurvey', fontsize=10)
ax2.set_ylabel('Residuals', fontsize=10)
ax2.set_title('Low Stress: StressSurvey vs Residuals', fontsize=11, fontweight='bold')
ax2.grid(True, alpha=0.3)

# Plot 3: Time vs Residuals (Low Stress)
ax3 = fig.add_subplot(2, 3, 3)
ax3.scatter(subset_low['Time'], residuals_low, color='blue', s=80, alpha=0.7, edgecolors='black', linewidth=1)
ax3.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax3.set_xlabel('Time', fontsize=10)
ax3.set_ylabel('Residuals', fontsize=10)
ax3.set_title('Low Stress: Time vs Residuals', fontsize=11, fontweight='bold')
ax3.grid(True, alpha=0.3)

# ===== HIGH STRESS SUBSET DIAGNOSTICS =====
# Plot 4: Residuals vs Fitted (High Stress)
ax4 = fig.add_subplot(2, 3, 4)
fitted_high = model_high.fittedvalues
residuals_high = model_high.resid
ax4.scatter(fitted_high, residuals_high, color='darkgreen', s=80, alpha=0.7, edgecolors='black', linewidth=1)
ax4.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax4.set_xlabel('Fitted Values', fontsize=10)
ax4.set_ylabel('Residuals', fontsize=10)
ax4.set_title('High Stress: Residuals vs Fitted\n(Checking Linearity)', fontsize=11, fontweight='bold')
ax4.grid(True, alpha=0.3)

# Plot 5: StressSurvey vs Residuals (High Stress)
ax5 = fig.add_subplot(2, 3, 5)
ax5.scatter(subset_high['StressSurvey'], residuals_high, color='darkgreen', s=80, alpha=0.7, edgecolors='black', linewidth=1)
ax5.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax5.set_xlabel('StressSurvey', fontsize=10)
ax5.set_ylabel('Residuals', fontsize=10)
ax5.set_title('High Stress: StressSurvey vs Residuals', fontsize=11, fontweight='bold')
ax5.grid(True, alpha=0.3)

# Plot 6: Time vs Residuals (High Stress)
ax6 = fig.add_subplot(2, 3, 6)
ax6.scatter(subset_high['Time'], residuals_high, color='darkgreen', s=80, alpha=0.7, edgecolors='black', linewidth=1)
ax6.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax6.set_xlabel('Time', fontsize=10)
ax6.set_ylabel('Residuals', fontsize=10)
ax6.set_title('High Stress: Time vs Residuals', fontsize=11, fontweight='bold')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('subset_linearity_diagnostics.png', dpi=150, bbox_inches='tight')
plt.show()


```

### Interpretation of Linearity Diagnostics

---

**INTERPRETATION:**

The residual plots show that in the low-stress subset, the errors are essentially zero across all fitted values, meaning the model fits that part of the data almost perfectly. In the high-stress subset, the residuals spread out above and below zero, showing that the model has trouble capturing the relationship when StressSurvey stops scaling cleanly with Stress. This contrast highlights that the model works well when the proxy is valid but breaks down when the proxy becomes distorted.

---

### Multiple Regression: Low Stress Subset (StressSurvey < 6)

```{python}
#| echo: false
# Fit multiple regression for low stress subset
X_low = subset_low[['StressSurvey', 'Time']]
y_low = subset_low['Anxiety']
X_low_sm = sm.add_constant(X_low)
model_low = sm.OLS(y_low, X_low_sm).fit()

print("MULTIPLE REGRESSION: Low Stress Subset (StressSurvey < 6)")
print("=" * 70)
print(f"Number of observations: {len(subset_low)}")
print("\n")
print(model_low.summary())
```

### Multiple Regression: High Stress Subset (StressSurvey >= 6)

```{python}
#| echo: false
# Fit multiple regression for high stress subset
X_high = subset_high[['StressSurvey', 'Time']]
y_high = subset_high['Anxiety']
X_high_sm = sm.add_constant(X_high)
model_high = sm.OLS(y_high, X_high_sm).fit()

print("MULTIPLE REGRESSION: High Stress Subset (StressSurvey >= 6)")
print("=" * 70)
print(f"Number of observations: {len(subset_high)}")
print("\n")
print(model_high.summary())
```

### Comparison of Coefficients

```{python}
#| echo: false
# Compare coefficients between the two subsets
print("COEFFICIENT COMPARISON BETWEEN SUBSETS:")
print("=" * 70)
print("\nLow Stress Subset (StressSurvey < 6):")
print(f"  Intercept: {model_low.params['const']:.4f}")
print(f"  StressSurvey: {model_low.params['StressSurvey']:.4f}")
print(f"  Time: {model_low.params['Time']:.4f}")
print(f"  R-squared: {model_low.rsquared:.4f}")

print("\nHigh Stress Subset (StressSurvey >= 6):")
print(f"  Intercept: {model_high.params['const']:.4f}")
print(f"  StressSurvey: {model_high.params['StressSurvey']:.4f}")
print(f"  Time: {model_high.params['Time']:.4f}")
print(f"  R-squared: {model_high.rsquared:.4f}")

print("\n" + "=" * 70)
print("KEY OBSERVATIONS:")
print(f"  Difference in Time coefficient: {model_low.params['Time'] - model_high.params['Time']:.4f}")
print(f"  Difference in StressSurvey coefficient: {model_low.params['StressSurvey'] - model_high.params['StressSurvey']:.4f}")
```

### Interpretation of Coefficient Comparison

---

**INTERPRETATION:**

I wanted to dig a bit deeper into these results and assess the output of the multiple regression. In the low-stress subset, the results came out exactly as hoped. The coefficients are statistically significant and match the true relationship almost perfectly, with StressSurvey at 0.333 which corresponds to a true Stress effect of 1 and Time right at 0.1. That was exciting to see because it shows the model works when the proxy behaves properly. In the high-stress subset, the coefficients are still statistically significant, but they drift far from the truth, with StressSurvey inflated and Time actually flipping negative. Seeing both side by side really highlights that significance on its own can be misleading, while in the right conditions the regression aligns perfectly with the real process.

---