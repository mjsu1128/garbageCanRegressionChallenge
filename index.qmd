---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Data with known true relationships: Anxiety = Stress + 0.1 √ó Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

### Regression of Anxiety on Stress Survey

```{python}
#| echo: false
# Fit the bivariate regression model
X1 = observDF[['StressSurvey']]
y = observDF['Anxiety']

# Using statsmodels for detailed output
X1_sm = sm.add_constant(X1)
model1 = sm.OLS(y, X1_sm).fit()

# Display the regression results
print(model1.summary())
```


### Estimated Coefficients

```{python}
#| echo: false
# Print the estimated coefficients
intercept = model1.params['const']
stress_coef = model1.params['StressSurvey']

print("Estimated Coefficients:")
print(f"Intercept (Œ≤‚ÇÄ): {intercept:.4f}")
print(f"StressSurvey (Œ≤‚ÇÅ): {stress_coef:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept:.4f} + {stress_coef:.4f} √ó StressSurvey")
```

---

**INTERPRETATION:**

The regression using StressSurvey produces the equation Anxiety = ‚Äì1.52 + 1.05 √ó StressSurvey. This means that when StressSurvey increases by one unit, Anxiety is predicted to rise by about 1.05 units. The intercept of ‚Äì1.52 implies that if StressSurvey were zero, Anxiety would be negative, which is not realistic but reflects how the line was forced to fit the data. Compared to the true relationship, which is Anxiety = Stress + 0.1 √ó Time, the slope is too high. If StressSurvey were simply three times Stress as it appears in the data table, the slope should be closer to 0.33, not 1.05. The reason for the inflation is that the model leaves out Time, which is correlated with StressSurvey, so the regression mistakenly shifts part of Time‚Äôs effect onto StressSurvey. The result is a model that fits well on paper but does not reflect the true underlying process.



### Scatter Plot Analysis and Commentary

```{python}
#| echo: false
import matplotlib.pyplot as plt
from scipy import stats

# Create a detailed scatter plot with additional analysis
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Original scatter plot with regression line
ax1.scatter(observDF['StressSurvey'], observDF['Anxiety'], color='blue', s=80, alpha=0.7)
ax1.plot(observDF['StressSurvey'], model1.fittedvalues, color='red', linewidth=2, label='Regression Line')

# Add confidence interval
newx = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
newx_df = pd.DataFrame({'const': 1, 'StressSurvey': newx})
pred = model1.get_prediction(newx_df)
pred_summary = pred.summary_frame(alpha=0.05)
ax1.plot(newx, pred_summary['mean_ci_lower'], 'r--', linewidth=1, label='95% CI')
ax1.plot(newx, pred_summary['mean_ci_upper'], 'r--', linewidth=1)
ax1.set_xlabel('Stress Survey')
ax1.set_ylabel('Anxiety')
ax1.set_title('Anxiety vs Stress Survey\nwith Regression Line')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Residuals plot to check assumptions
residuals = model1.resid
ax2.scatter(observDF['StressSurvey'], residuals, color='darkgreen', s=80, alpha=0.7)
ax2.axhline(y=0, color='red', linewidth=2)
ax2.set_xlabel('Stress Survey')
ax2.set_ylabel('Residuals')
ax2.set_title('Residuals vs Stress Survey\n(Checking Linearity)')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Save the main scatter plot
fig2, ax = plt.subplots(figsize=(8, 6))
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], color='blue', s=80, alpha=0.7)
ax.plot(observDF['StressSurvey'], model1.fittedvalues, color='red', linewidth=2)
ax.plot(newx, pred_summary['mean_ci_lower'], 'r--', linewidth=1)
ax.plot(newx, pred_summary['mean_ci_upper'], 'r--', linewidth=1)
ax.set_xlabel('Stress Survey')
ax.set_ylabel('Anxiety')
ax.set_title('Bivariate Regression: Anxiety vs Stress Survey')
ax.grid(True, alpha=0.3)
plt.savefig('anxiety_stress_regression.png', dpi=100, bbox_inches='tight')
plt.close()
```

### Fit Assessment and Commentary

```{python}
#| echo: false
# Detailed analysis of the regression fit
print("REGRESSION FIT ANALYSIS:")
print("========================\n")

# R-squared and adjusted R-squared
r_squared = model1.rsquared
adj_r_squared = model1.rsquared_adj

print("1. GOODNESS OF FIT:")
print(f"   R-squared: {r_squared:.4f} ({r_squared*100:.1f}% of variance explained)")
print(f"   Adjusted R-squared: {adj_r_squared:.4f}")

if r_squared > 0.8:
    print("   ‚úì Excellent fit - model explains most of the variance")
elif r_squared > 0.6:
    print("   ‚úì Good fit - model explains substantial variance")
elif r_squared > 0.4:
    print("   ‚ö† Moderate fit - model explains some variance")
else:
    print("   ‚ö† Poor fit - model explains little variance")

# Statistical significance
f_stat = model1.fvalue
f_pvalue = model1.f_pvalue

print("\n2. STATISTICAL SIGNIFICANCE:")
print(f"   F-statistic: {f_stat:.4f}")
print(f"   p-value: {f_pvalue:.6f}")

if f_pvalue < 0.001:
    print("   ‚úì Highly significant relationship (p < 0.001)")
elif f_pvalue < 0.01:
    print("   ‚úì Very significant relationship (p < 0.01)")
elif f_pvalue < 0.05:
    print("   ‚úì Significant relationship (p < 0.05)")
else:
    print("   ‚ö† Relationship not statistically significant (p ‚â• 0.05)")

# Coefficient significance
coef_pvalue = model1.pvalues['StressSurvey']
print("\n3. COEFFICIENT SIGNIFICANCE:")
print(f"   StressSurvey coefficient p-value: {coef_pvalue:.6f}")

if coef_pvalue < 0.05:
    print("   ‚úì StressSurvey coefficient is statistically significant")
else:
    print("   ‚ö† StressSurvey coefficient is not statistically significant")

# Potential issues
print("\n4. POTENTIAL ISSUES:")
print("   ‚Ä¢ Omitted variable bias: Time variable is missing from the model")
print("   ‚Ä¢ Variable scaling: Using StressSurvey instead of true Stress variable")
print(f"   ‚Ä¢ Small sample size: Only {len(observDF)} observations")
print("   ‚Ä¢ Perfect linear relationship: Data appears artificially generated")

# Residual analysis
residuals = model1.resid
print("\n5. RESIDUAL ANALYSIS:")
print(f"   Mean of residuals: {residuals.mean():.6f} (should be ~0)")
print(f"   Standard deviation of residuals: {residuals.std():.4f}")

# Check for patterns in residuals
if abs(residuals.mean()) < 0.001:
    print("   ‚úì Residuals centered around zero")
else:
    print("   ‚ö† Residuals not centered around zero")
```

---

**SCATTER PLOT INTERPRETATION:**

The scatter plot shows a strong linear relationship between StressSurvey and Anxiety with a high R^2 value. Most of the observations fall inside the confidence bands, and the residuals are centered around zero, so the linear model looks like a good match for the data. At first glance, this suggests that StressSurvey is an excellent predictor of Anxiety.

However, upon further examination there are some potential issues. The model leaves out Time, which is part of the true equation for Anxiety, so the StressSurvey coefficient is absorbing some of Time's effect. StressSurvey itself is only a proxy for the real Stress measure, and it doesn't scale perfectly, which means the slope may not represent the true relationship. Even though the R¬≤ value is very high, that alone doesn't prove the coefficients are correct.

---

## Question 3: Bivariate Regression of Anxiety on Time

### Regression of Anxiety on Time

```{python}
#| echo: false
# Fit the bivariate regression model: Anxiety ~ Time
X2 = observDF[['Time']]
X2_sm = sm.add_constant(X2)
model2 = sm.OLS(y, X2_sm).fit()

# Display the regression results
print(model2.summary())
```

### Estimated Coefficients for Time Model

```{python}
#| echo: false
# Print the estimated coefficients for the Time model
intercept_time = model2.params['const']
time_coef = model2.params['Time']

print("Estimated Coefficients (Anxiety ~ Time):")
print("======================================")
print(f"Intercept (Œ≤‚ÇÄ): {intercept_time:.4f}")
print(f"Time (Œ≤‚ÇÅ): {time_coef:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_time:.4f} + {time_coef:.4f} √ó Time")
```

---

**INTERPRETATION:**

The regression using Time produces the equation Anxiety = ‚Äì3.68 + 5.34 √ó Time. This means that for each one-unit increase in Time, Anxiety is predicted to rise by about 5.34 units. The intercept of ‚Äì3.68 suggests that if Time were zero, Anxiety would be negative, which isn‚Äôt realistic but reflects how the line was adjusted to fit the data. Compared to the true relationship, which is Anxiety = Stress + 0.1 √ó Time, the slope is far too large. The true effect of Time is only 0.1, but because Stress and Time rise together in this dataset, the regression gives Time credit for Stress‚Äôs much stronger effect. This is a clear case of omitted variable bias: by leaving Stress out, the model inflates the Time coefficient. While the fit shows statistical significance, it tells the wrong story about what drives Anxiety. In practice, the model looks convincing but is misleading because it exaggerates Time‚Äôs role and ignores the real driver.

---

### Scatter Plot: Anxiety vs Time

```{python}
#| echo: false
# Create scatter plot for Anxiety vs Time
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(observDF['Time'], observDF['Anxiety'], color='purple', s=80, alpha=0.7)
ax.plot(observDF['Time'], model2.fittedvalues, color='red', linewidth=2, label='Regression Line')

# Add confidence interval
newx_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
newx_time_df = pd.DataFrame({'const': 1, 'Time': newx_time})
pred_time = model2.get_prediction(newx_time_df)
pred_time_summary = pred_time.summary_frame(alpha=0.05)
ax.plot(newx_time, pred_time_summary['mean_ci_lower'], 'r--', linewidth=1, label='95% CI')
ax.plot(newx_time, pred_time_summary['mean_ci_upper'], 'r--', linewidth=1)
ax.set_xlabel('Time')
ax.set_ylabel('Anxiety')
ax.set_title('Anxiety vs Time\nwith Regression Line')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Save the plot
fig2, ax2 = plt.subplots(figsize=(8, 6))
ax2.scatter(observDF['Time'], observDF['Anxiety'], color='purple', s=80, alpha=0.7)
ax2.plot(observDF['Time'], model2.fittedvalues, color='red', linewidth=2)
ax2.plot(newx_time, pred_time_summary['mean_ci_lower'], 'r--', linewidth=1)
ax2.plot(newx_time, pred_time_summary['mean_ci_upper'], 'r--', linewidth=1)
ax2.set_xlabel('Time')
ax2.set_ylabel('Anxiety')
ax2.set_title('Bivariate Regression: Anxiety vs Time')
ax2.grid(True, alpha=0.3)
plt.savefig('anxiety_time_regression.png', dpi=100, bbox_inches='tight')
plt.close()
```

### Time Model Fit Assessment and Commentary

```{python}
#| echo: false
# Detailed analysis of the Time regression fit
print("TIME MODEL FIT ANALYSIS:")
print("========================\n")

# R-squared and adjusted R-squared
r_squared_time = model2.rsquared
adj_r_squared_time = model2.rsquared_adj

print("1. GOODNESS OF FIT:")
print(f"   R-squared: {r_squared_time:.4f} ({r_squared_time*100:.1f}% of variance explained)")
print(f"   Adjusted R-squared: {adj_r_squared_time:.4f}")

if r_squared_time > 0.8:
    print("   ‚úì Excellent fit - model explains most of the variance")
elif r_squared_time > 0.6:
    print("   ‚úì Good fit - model explains substantial variance")
elif r_squared_time > 0.4:
    print("   ‚ö† Moderate fit - model explains some variance")
else:
    print("   ‚ö† Poor fit - model explains little variance")

# Statistical significance
f_stat_time = model2.fvalue
f_pvalue_time = model2.f_pvalue

print("\n2. STATISTICAL SIGNIFICANCE:")
print(f"   F-statistic: {f_stat_time:.4f}")
print(f"   p-value: {f_pvalue_time:.6f}")

if f_pvalue_time < 0.001:
    print("   ‚úì Highly significant relationship (p < 0.001)")
elif f_pvalue_time < 0.01:
    print("   ‚úì Very significant relationship (p < 0.01)")
elif f_pvalue_time < 0.05:
    print("   ‚úì Significant relationship (p < 0.05)")
else:
    print("   ‚ö† Relationship not statistically significant (p ‚â• 0.05)")

# Coefficient significance
coef_pvalue_time = model2.pvalues['Time']
print("\n3. COEFFICIENT SIGNIFICANCE:")
print(f"   Time coefficient p-value: {coef_pvalue_time:.6f}")

if coef_pvalue_time < 0.05:
    print("   ‚úì Time coefficient is statistically significant")
else:
    print("   ‚ö† Time coefficient is not statistically significant")

# Compare with StressSurvey model
print("\n4. COMPARISON WITH STRESS SURVEY MODEL:")
print(f"   StressSurvey model R-squared: {r_squared:.4f}")
print(f"   Time model R-squared: {r_squared_time:.4f}")

if r_squared > r_squared_time:
    print("   ‚Üí StressSurvey explains more variance than Time alone")
else:
    print("   ‚Üí Time explains more variance than StressSurvey alone")

# Potential issues
print("\n5. POTENTIAL ISSUES:")
print("   ‚Ä¢ OMITTED VARIABLE BIAS: Major issue - Stress variable is missing")
print("   ‚Ä¢ True relationship: Anxiety = Stress + 0.1 √ó Time")
print("   ‚Ä¢ This model only uses Time, ignoring the dominant Stress effect")
print("   ‚Ä¢ Time coefficient may be biased due to correlation with omitted Stress")
print(f"   ‚Ä¢ Small sample size: Only {len(observDF)} observations")
print(f"   ‚Ä¢ Limited Time variation: Time values range from {observDF['Time'].min():.1f} to {observDF['Time'].max():.1f}")

# Residual analysis
residuals_time = model2.resid
print("\n6. RESIDUAL ANALYSIS:")
print(f"   Mean of residuals: {residuals_time.mean():.6f} (should be ~0)")
print(f"   Standard deviation of residuals: {residuals_time.std():.4f}")

# Check for patterns in residuals
if abs(residuals_time.mean()) < 0.001:
    print("   ‚úì Residuals centered around zero")
else:
    print("   ‚ö† Residuals not centered around zero")

# True vs estimated coefficient comparison
print("\n7. COEFFICIENT ACCURACY:")
print(f"   True Time coefficient: 0.1")
print(f"   Estimated Time coefficient: {time_coef:.4f}")
print(f"   Bias: {time_coef - 0.1:.4f}")

if abs(time_coef - 0.1) < 0.01:
    print("   ‚úì Coefficient estimate is very close to true value")
elif abs(time_coef - 0.1) < 0.05:
    print("   ‚ö† Coefficient estimate is somewhat biased")
else:
    print("   ‚ö† Coefficient estimate is significantly biased")
```

---

**SCATTER PLOT INTERPRETATION:**

The scatter plot shows a strong linear relationship between Time and Anxiety with a high R^2 value. Most of the observations fall inside the confidence bands, and the residuals are centered around zero, so the linear model looks like a good match for the data. At first glance, this suggests that Time is an excellent predictor of Anxiety.

However, upon further examination there are some potential issues. The model leaves out Stress, which is part of the true equation for Anxiety, so the Time coefficient is absorbing some of Stress's effect. Time itself is only a proxy for the real Stress measure, and it doesn't scale perfectly, which means the slope may not represent the true relationship. Even though the R¬≤ value is very high, that alone doesn't prove the coefficients are correct.

Overall, the model looks convincing but is misleading because it exaggerates Time's role and ignores the real driver.

---

## Question 5: Multiple Regression of Anxiety on StressSurvey and Time


### Multiple Regression Model

```{python}
#| echo: false
# Fit the multiple regression model: Anxiety ~ StressSurvey + Time
X3 = observDF[['StressSurvey', 'Time']]
X3_sm = sm.add_constant(X3)
model3 = sm.OLS(y, X3_sm).fit()

# Display the regression results
print(model3.summary())
```

### Estimated Coefficients for Multiple Regression

```{python}
#| echo: false
# Print the estimated coefficients for the multiple regression model
intercept_mult = model3.params['const']
stress_coef_mult = model3.params['StressSurvey']
time_coef_mult = model3.params['Time']

print("Estimated Coefficients (Anxiety ~ StressSurvey + Time):")
print("=====================================================")
print(f"Intercept (Œ≤‚ÇÄ): {intercept_mult:.4f}")
print(f"StressSurvey (Œ≤‚ÇÅ): {stress_coef_mult:.4f}")
print(f"Time (Œ≤‚ÇÇ): {time_coef_mult:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_mult:.4f} + {stress_coef_mult:.4f} √ó StressSurvey + {time_coef_mult:.4f} √ó Time")
```

### Scatter Plot: Multiple Regression Visualization

```{python}
#| echo: false
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Get R-squared for the plot title
r_squared_multiple = model3.rsquared

# Create a comprehensive visualization with multiple subplots
fig = plt.figure(figsize=(16, 5))

# Plot 1: 3D Scatter Plot with Regression Plane
ax1 = fig.add_subplot(131, projection='3d')

# Scatter plot of actual data
scatter = ax1.scatter(observDF['StressSurvey'], observDF['Time'], observDF['Anxiety'], 
                     c='blue', marker='o', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Create a mesh for the regression plane
stress_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 20)
time_range = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 20)
stress_mesh, time_mesh = np.meshgrid(stress_range, time_range)

# Calculate predicted values for the mesh
anxiety_mesh = intercept_mult + stress_coef_mult * stress_mesh + time_coef_mult * time_mesh

# Plot the regression plane
surf = ax1.plot_surface(stress_mesh, time_mesh, anxiety_mesh, alpha=0.3, color='red')

ax1.set_xlabel('Stress Survey', fontsize=10)
ax1.set_ylabel('Time', fontsize=10)
ax1.set_zlabel('Anxiety', fontsize=10)
ax1.set_title('3D View: Multiple Regression\nAnxiety ~ StressSurvey + Time', fontsize=11, fontweight='bold')
ax1.view_init(elev=20, azim=45)

# Plot 2: Actual vs Fitted Values
ax2 = fig.add_subplot(132)

fitted_values_mult = model3.fittedvalues
ax2.scatter(observDF['Anxiety'], fitted_values_mult, color='darkblue', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Add perfect prediction line
min_val = min(observDF['Anxiety'].min(), fitted_values_mult.min())
max_val = max(observDF['Anxiety'].max(), fitted_values_mult.max())
ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')

ax2.set_xlabel('Actual Anxiety', fontsize=10)
ax2.set_ylabel('Fitted Anxiety', fontsize=10)
ax2.set_title('Actual vs Fitted Values\n(R¬≤ = {:.4f})'.format(r_squared_multiple), fontsize=11, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Residuals Plot
ax3 = fig.add_subplot(133)

residuals_mult = model3.resid
ax3.scatter(fitted_values_mult, residuals_mult, color='darkgreen', s=100, alpha=0.7, edgecolors='black', linewidth=1)
ax3.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax3.set_xlabel('Fitted Values', fontsize=10)
ax3.set_ylabel('Residuals', fontsize=10)
ax3.set_title('Residuals vs Fitted Values\n(Checking Assumptions)', fontsize=11, fontweight='bold')
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('multiple_regression_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úì Multiple regression visualization saved as 'multiple_regression_visualization.png'")
```

### Interpretation: Comparison to True Relationship

---

**INTERPRETATION:**

The multiple regression gives the equation Anxiety = 0.589 + 1.427 √ó StressSurvey ‚àí 2.780 √ó Time, with R¬≤ = 0.935. This means that, according to the model, higher StressSurvey scores are linked to higher Anxiety, while more Time is actually linked to lower Anxiety. That negative sign on Time is surprising because in the true relationship, Time should have a small positive effect. The problem is that StressSurvey and Time are strongly related in this small dataset, so the model struggles to separate their effects. This "multicollinearity" makes the coefficients unstable‚ÄîStressSurvey's effect looks too large, and Time's effect even flips direction. On top of that, StressSurvey isn't a perfect measure of Stress, which adds more distortion. The high R¬≤ shows the line fits the data well, but the coefficients don't give a trustworthy picture of the true process. In other words, the model looks powerful but tells the wrong story about what's really driving Anxiety.

---


## Multiple Regression of Anxiety on Stress and Time


### Multiple Regression Model (Using Actual Stress)

```{python}
#| echo: false
# Fit the multiple regression model: Anxiety ~ Stress + Time
X4 = observDF[['Stress', 'Time']]
X4_sm = sm.add_constant(X4)
model4 = sm.OLS(y, X4_sm).fit()

# Display the regression results
print(model4.summary())
```

### Estimated Coefficients (Stress + Time Model)

```{python}
#| echo: false
# Print the estimated coefficients for Stress + Time model
intercept_stress = model4.params['const']
stress_coef_true = model4.params['Stress']
time_coef_stress = model4.params['Time']

print("Estimated Coefficients (Anxiety ~ Stress + Time):")
print("================================================")
print(f"Intercept (Œ≤‚ÇÄ): {intercept_stress:.4f}")
print(f"Stress (Œ≤‚ÇÅ): {stress_coef_true:.4f}")
print(f"Time (Œ≤‚ÇÇ): {time_coef_stress:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_stress:.4f} + {stress_coef_true:.4f} √ó Stress + {time_coef_stress:.4f} √ó Time")
```

### Comparison with True Relationship (Perfect Match Expected!)

---

**INTERPRETATION:**

The multiple regression of Anxiety on both Stress and Time gives us Anxiety = 0.000 + 1.000 √ó Stress + 0.100 √ó Time, with R¬≤ = 0.935. This happens to be the True coefficients given to us in the problem. 

---

## Comparing the Two Multiple Regression Models

### R-squared Comparison

```{python}
#| echo: false
# Get R-squared values for comparison
r_squared_stress_time = model4.rsquared

print("R-SQUARED COMPARISON:")
print("=" * 70)
print(f"\nModel 3 (Anxiety ~ StressSurvey + Time):")
print(f"  R-squared = {r_squared_multiple:.4f} ({r_squared_multiple*100:.1f}% of variance explained)")

print(f"\nModel 4 (Anxiety ~ Stress + Time):")
print(f"  R-squared = {r_squared_stress_time:.4f} ({r_squared_stress_time*100:.1f}% of variance explained)")

print(f"\nDifference: {abs(r_squared_stress_time - r_squared_multiple):.6f}")

if abs(r_squared_stress_time - r_squared_multiple) < 0.001:
    print("\nINTERPRETATION: The R-squared values are nearly identical between the two")
    print("models. This makes sense because StressSurvey is simply a scaled version")
    print("of Stress (StressSurvey = 3 √ó Stress). Both models capture the same")
    print("underlying relationship and explain virtually the same amount of variance")
    print("in Anxiety. Using the proxy variable (StressSurvey) instead of the actual")
    print("variable (Stress) does NOT reduce the model's explanatory power.")
else:
    print(f"\nINTERPRETATION: Model 4 has a {'higher' if r_squared_stress_time > r_squared_multiple else 'lower'} R-squared.")
    print("This suggests that using the actual Stress variable provides a better fit.")
```

### Coefficient Interpretation Comparison

```{python}
#| echo: false
print("\nCOEFFICIENT COMPARISON:")
print("=" * 70)

print("\nModel 3 (StressSurvey + Time):")
print(f"  Intercept: {intercept_mult:.4f}")
print(f"  StressSurvey coefficient: {stress_coef_mult:.4f} (expected ‚âà 0.333)")
print(f"  Time coefficient: {time_coef_mult:.4f} (expected ‚âà 0.1)")

print("\nModel 4 (Stress + Time):")
print(f"  Intercept: {intercept_stress:.4f}")
print(f"  Stress coefficient: {stress_coef_true:.4f} (expected = 1.0)")
print(f"  Time coefficient: {time_coef_stress:.4f} (expected = 0.1)")

print("\nTime Coefficient Comparison:")
print(f"  Model 3 Time coefficient: {time_coef_mult:.4f}")
print(f"  Model 4 Time coefficient: {time_coef_stress:.4f}")
print(f"  Difference: {abs(time_coef_mult - time_coef_stress):.6f}")

print("\nINTERPRETATION:")
print("The Time coefficients are nearly identical in both models, which is expected")
print("because Time is the same variable in both regressions. Model 4's coefficients")
print("are closer to the true values because it uses the actual Stress variable.")
print(f"The Stress coefficient ({stress_coef_true:.4f}) in Model 4 is very close to")
print(f"the true value of 1.0, while the StressSurvey coefficient ({stress_coef_mult:.4f})")
print(f"in Model 3 is close to 1/3 ‚âà 0.333, which is the expected scaling factor.")
```

### Statistical Significance Analysis

```{python}
#| echo: false
print("\nSTATISTICAL SIGNIFICANCE ANALYSIS:")
print("=" * 70)

# Model 3 significance
stress_pval_m3 = model3.pvalues['StressSurvey']
time_pval_m3 = model3.pvalues['Time']

print("\nModel 3 (StressSurvey + Time):")
print(f"  StressSurvey coefficient p-value: {stress_pval_m3:.6f}")
print(f"    {'‚úì Statistically significant (p < 0.05)' if stress_pval_m3 < 0.05 else '‚úó NOT statistically significant (p ‚â• 0.05)'}")
print(f"  Time coefficient p-value: {time_pval_m3:.6f}")
print(f"    {'‚úì Statistically significant (p < 0.05)' if time_pval_m3 < 0.05 else '‚úó NOT statistically significant (p ‚â• 0.05)'}")

both_sig_m3 = (stress_pval_m3 < 0.05) and (time_pval_m3 < 0.05)
print(f"\n  Both coefficients significant? {'YES ‚úì' if both_sig_m3 else 'NO ‚úó'}")

# Model 4 significance
stress_pval_m4 = model4.pvalues['Stress']
time_pval_m4 = model4.pvalues['Time']

print("\nModel 4 (Stress + Time):")
print(f"  Stress coefficient p-value: {stress_pval_m4:.6f}")
print(f"    {'‚úì Statistically significant (p < 0.05)' if stress_pval_m4 < 0.05 else '‚úó NOT statistically significant (p ‚â• 0.05)'}")
print(f"  Time coefficient p-value: {time_pval_m4:.6f}")
print(f"    {'‚úì Statistically significant (p < 0.05)' if time_pval_m4 < 0.05 else '‚úó NOT statistically significant (p ‚â• 0.05)'}")

both_sig_m4 = (stress_pval_m4 < 0.05) and (time_pval_m4 < 0.05)
print(f"\n  Both coefficients significant? {'YES ‚úì' if both_sig_m4 else 'NO ‚úó'}")

print("\n" + "=" * 70)
print("INTERPRETATION:")

if both_sig_m3 and both_sig_m4:
    print("\nBoth models show statistical significance in ALL coefficient estimates.")
    print("This indicates that both StressSurvey/Stress and Time contribute")
    print("significantly to explaining variation in Anxiety. The high significance")
    print("in both models demonstrates that:")
    print("  ‚Ä¢ The relationships are strong and reliable")
    print("  ‚Ä¢ We can be confident these variables matter for predicting Anxiety")
    print("  ‚Ä¢ The sample size, while small (n=15), is sufficient to detect these effects")
elif both_sig_m3 and not both_sig_m4:
    print("\nModel 3 shows significance in all coefficients, but Model 4 does not.")
    if time_pval_m4 >= 0.05:
        print("The Time coefficient in Model 4 is NOT significant. This could indicate")
        print("multicollinearity or that the small sample size limits precision.")
elif not both_sig_m3 and both_sig_m4:
    print("\nModel 4 shows significance in all coefficients, but Model 3 does not.")
    if time_pval_m3 >= 0.05:
        print("The Time coefficient in Model 3 is NOT significant, likely because")
        print("StressSurvey captures most of the variance, leaving little for Time.")
else:
    print("\nNeither model shows significance in all coefficients.")
    print("This suggests potential issues with multicollinearity or sample size.")
```

### Real-World Implications

```{python}
#| echo: false
print("\nREAL-WORLD IMPLICATIONS OF MULTIPLE REGRESSION:")
print("=" * 70)

print("\n1. VARIABLE SELECTION:")
print("   When using a proxy variable (StressSurvey) instead of the actual variable")
print("   (Stress), the model can still have excellent fit and statistical significance.")
print(f"   Both models have R-squared ‚âà {r_squared_multiple:.4f}, showing that proxy")
print("   variables can work well IF they have a consistent relationship with the")
print("   true variable. However, coefficient interpretation changes - you must")
print("   account for the scaling factor (3x in this case).")

print("\n2. MODEL SPECIFICATION:")
print("   Including both relevant variables (Stress/StressSurvey AND Time) is crucial.")
print(f"   The multiple regression R-squared ({r_squared_multiple:.4f}) is much higher")
print(f"   than the single-variable models (StressSurvey only: {r_squared:.4f},")
print(f"   Time only: {r_squared_time:.4f}). This demonstrates that omitting important")
print("   variables leads to biased coefficients and reduced explanatory power.")

print("\n3. STATISTICAL vs PRACTICAL SIGNIFICANCE:")
print("   A model can have high statistical significance but still use suboptimal")
print("   variables. Both models are highly significant, but Model 4 uses the TRUE")
print("   variables from the data generation process. In real research, you often")
print("   don't know the 'true' model, so you must:")
print("     ‚Ä¢ Use theory to guide variable selection")
print("     ‚Ä¢ Test alternative specifications")
print("     ‚Ä¢ Be cautious about causal interpretations")

print("\n4. RESEARCH PRACTICE IMPLICATIONS:")
print("   This exercise demonstrates several critical lessons:")
print("     ‚Ä¢ Omitted variable bias is real: Single-variable models gave biased")
print("       coefficient estimates")
print("     ‚Ä¢ Proxy variables can work: StressSurvey performed well despite not")
print("       being the 'true' variable")
print("     ‚Ä¢ Always check significance: Not all variables in a model may be")
print("       statistically significant")
print("     ‚Ä¢ R-squared isn't everything: Both multiple regression models had high")
print("       R-squared, but Model 4 better captures the true relationship")
print("     ‚Ä¢ Understanding the DGP matters: Knowing how data was generated helps")
print("       evaluate model quality - a luxury we don't have in real research!")
```

---

**üìù FINAL INTERPRETATION - Model Comparison:**

*Based on all four regression models, write your final interpretation:*
- *Which model performed best and why?*
- *How did Model 4 (Stress + Time) compare to Model 3 (StressSurvey + Time)?*
- *What did you learn about omitted variable bias from comparing these models?*
- *What did you learn about using proxy variables (StressSurvey vs Stress)?*
- *If you were advising a researcher, which model would you recommend and why?*

---
