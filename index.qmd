---
title: "Garbage Can Regression Challenge"
format:
  html: default
execute:
  echo: false
  eval: true
---

```{python}
#| echo: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Data with known true relationships: Anxiety = Stress + 0.1 × Time
data = {
    'Stress': [0,0,0,1,1,1,2,2,2,8,8,8,12,12,12],
    'StressSurvey': [0,0,0,3,3,3,6,6,6,9,9,9,12,12,12],
    'Time': [0,1,1,1,1,1,2,2,2,2,2,2.1,2.2,2.2,2.2],
    'Anxiety': [0,0.1,0.1,1.1,1.1,1.1,2.2,2.2,2.2,8.2,8.2,8.21,12.22,12.22,12.22]
}

observDF = pd.DataFrame(data)
print(observDF)
```

### Regression of Anxiety on Stress Survey

```{python}
#| echo: false
# Fit the bivariate regression model
X1 = observDF[['StressSurvey']]
y = observDF['Anxiety']

# Using statsmodels for detailed output
X1_sm = sm.add_constant(X1)
model1 = sm.OLS(y, X1_sm).fit()

# Display the regression results
print(model1.summary())
```


### Estimated Coefficients

```{python}
#| echo: false
# Print the estimated coefficients
intercept = model1.params['const']
stress_coef = model1.params['StressSurvey']

print("Estimated Coefficients:")
print(f"Intercept (β₀): {intercept:.4f}")
print(f"StressSurvey (β₁): {stress_coef:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept:.4f} + {stress_coef:.4f} × StressSurvey")
```

---

**INTERPRETATION:**

The regression using StressSurvey produces the equation Anxiety = –1.52 + 1.05 × StressSurvey. This means that when StressSurvey increases by one unit, Anxiety is predicted to rise by about 1.05 units. The intercept of –1.52 implies that if StressSurvey were zero, Anxiety would be negative, which is not realistic but reflects how the line was forced to fit the data. Compared to the true relationship, which is Anxiety = Stress + 0.1 × Time, the slope is too high. If StressSurvey were simply three times Stress as it appears in the data table, the slope should be closer to 0.33, not 1.05. The reason for the inflation is that the model leaves out Time, which is correlated with StressSurvey, so the regression mistakenly shifts part of Time’s effect onto StressSurvey. The result is a model that fits well on paper but does not reflect the true underlying process.



### Scatter Plot Analysis and Commentary

```{python}
#| echo: false
import matplotlib.pyplot as plt
from scipy import stats

# Create a detailed scatter plot with additional analysis
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Plot 1: Original scatter plot with regression line
ax1.scatter(observDF['StressSurvey'], observDF['Anxiety'], color='blue', s=80, alpha=0.7)
ax1.plot(observDF['StressSurvey'], model1.fittedvalues, color='red', linewidth=2, label='Regression Line')

# Add confidence interval
newx = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 100)
newx_df = pd.DataFrame({'const': 1, 'StressSurvey': newx})
pred = model1.get_prediction(newx_df)
pred_summary = pred.summary_frame(alpha=0.05)
ax1.plot(newx, pred_summary['mean_ci_lower'], 'r--', linewidth=1, label='95% CI')
ax1.plot(newx, pred_summary['mean_ci_upper'], 'r--', linewidth=1)
ax1.set_xlabel('Stress Survey')
ax1.set_ylabel('Anxiety')
ax1.set_title('Anxiety vs Stress Survey\nwith Regression Line')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Plot 2: Residuals plot to check assumptions
residuals = model1.resid
ax2.scatter(observDF['StressSurvey'], residuals, color='darkgreen', s=80, alpha=0.7)
ax2.axhline(y=0, color='red', linewidth=2)
ax2.set_xlabel('Stress Survey')
ax2.set_ylabel('Residuals')
ax2.set_title('Residuals vs Stress Survey\n(Checking Linearity)')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Save the main scatter plot
fig2, ax = plt.subplots(figsize=(8, 6))
ax.scatter(observDF['StressSurvey'], observDF['Anxiety'], color='blue', s=80, alpha=0.7)
ax.plot(observDF['StressSurvey'], model1.fittedvalues, color='red', linewidth=2)
ax.plot(newx, pred_summary['mean_ci_lower'], 'r--', linewidth=1)
ax.plot(newx, pred_summary['mean_ci_upper'], 'r--', linewidth=1)
ax.set_xlabel('Stress Survey')
ax.set_ylabel('Anxiety')
ax.set_title('Bivariate Regression: Anxiety vs Stress Survey')
ax.grid(True, alpha=0.3)
plt.savefig('anxiety_stress_regression.png', dpi=100, bbox_inches='tight')
plt.close()
```

### Fit Assessment and Commentary

```{python}
#| echo: false
# Detailed analysis of the regression fit
print("REGRESSION FIT ANALYSIS:")
print("========================\n")

# R-squared and adjusted R-squared
r_squared = model1.rsquared
adj_r_squared = model1.rsquared_adj

print("1. GOODNESS OF FIT:")
print(f"   R-squared: {r_squared:.4f} ({r_squared*100:.1f}% of variance explained)")
print(f"   Adjusted R-squared: {adj_r_squared:.4f}")

if r_squared > 0.8:
    print("   ✓ Excellent fit - model explains most of the variance")
elif r_squared > 0.6:
    print("   ✓ Good fit - model explains substantial variance")
elif r_squared > 0.4:
    print("   ⚠ Moderate fit - model explains some variance")
else:
    print("   ⚠ Poor fit - model explains little variance")

# Statistical significance
f_stat = model1.fvalue
f_pvalue = model1.f_pvalue

print("\n2. STATISTICAL SIGNIFICANCE:")
print(f"   F-statistic: {f_stat:.4f}")
print(f"   p-value: {f_pvalue:.6f}")

if f_pvalue < 0.001:
    print("   ✓ Highly significant relationship (p < 0.001)")
elif f_pvalue < 0.01:
    print("   ✓ Very significant relationship (p < 0.01)")
elif f_pvalue < 0.05:
    print("   ✓ Significant relationship (p < 0.05)")
else:
    print("   ⚠ Relationship not statistically significant (p ≥ 0.05)")

# Coefficient significance
coef_pvalue = model1.pvalues['StressSurvey']
print("\n3. COEFFICIENT SIGNIFICANCE:")
print(f"   StressSurvey coefficient p-value: {coef_pvalue:.6f}")

if coef_pvalue < 0.05:
    print("   ✓ StressSurvey coefficient is statistically significant")
else:
    print("   ⚠ StressSurvey coefficient is not statistically significant")

# Potential issues
print("\n4. POTENTIAL ISSUES:")
print("   • Omitted variable bias: Time variable is missing from the model")
print("   • Variable scaling: Using StressSurvey instead of true Stress variable")
print(f"   • Small sample size: Only {len(observDF)} observations")
print("   • Perfect linear relationship: Data appears artificially generated")

# Residual analysis
residuals = model1.resid
print("\n5. RESIDUAL ANALYSIS:")
print(f"   Mean of residuals: {residuals.mean():.6f} (should be ~0)")
print(f"   Standard deviation of residuals: {residuals.std():.4f}")

# Check for patterns in residuals
if abs(residuals.mean()) < 0.001:
    print("   ✓ Residuals centered around zero")
else:
    print("   ⚠ Residuals not centered around zero")
```

---

**SCATTER PLOT INTERPRETATION:**

The scatter plot shows a strong linear relationship between StressSurvey and Anxiety with a high R^2 value. Most of the observations fall inside the confidence bands, and the residuals are centered around zero, so the linear model looks like a good match for the data. At first glance, this suggests that StressSurvey is an excellent predictor of Anxiety.

However, upon further examination there are some potential issues. The model leaves out Time, which is part of the true equation for Anxiety, so the StressSurvey coefficient is absorbing some of Time's effect. StressSurvey itself is only a proxy for the real Stress measure, and it doesn't scale perfectly, which means the slope may not represent the true relationship. Even though the R² value is very high, that alone doesn't prove the coefficients are correct.

---

## Question 3: Bivariate Regression of Anxiety on Time

### Regression of Anxiety on Time

```{python}
#| echo: false
# Fit the bivariate regression model: Anxiety ~ Time
X2 = observDF[['Time']]
X2_sm = sm.add_constant(X2)
model2 = sm.OLS(y, X2_sm).fit()

# Display the regression results
print(model2.summary())
```

### Estimated Coefficients for Time Model

```{python}
#| echo: false
# Print the estimated coefficients for the Time model
intercept_time = model2.params['const']
time_coef = model2.params['Time']

print("Estimated Coefficients (Anxiety ~ Time):")
print("======================================")
print(f"Intercept (β₀): {intercept_time:.4f}")
print(f"Time (β₁): {time_coef:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_time:.4f} + {time_coef:.4f} × Time")
```

---

**INTERPRETATION:**

The regression using Time produces the equation Anxiety = –3.68 + 5.34 × Time. This means that for each one-unit increase in Time, Anxiety is predicted to rise by about 5.34 units. The intercept of –3.68 suggests that if Time were zero, Anxiety would be negative, which isn’t realistic but reflects how the line was adjusted to fit the data. Compared to the true relationship, which is Anxiety = Stress + 0.1 × Time, the slope is far too large. The true effect of Time is only 0.1, but because Stress and Time rise together in this dataset, the regression gives Time credit for Stress’s much stronger effect. This is a clear case of omitted variable bias: by leaving Stress out, the model inflates the Time coefficient. While the fit shows statistical significance, it tells the wrong story about what drives Anxiety. In practice, the model looks convincing but is misleading because it exaggerates Time’s role and ignores the real driver.

---

### Scatter Plot: Anxiety vs Time

```{python}
#| echo: false
# Create scatter plot for Anxiety vs Time
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(observDF['Time'], observDF['Anxiety'], color='purple', s=80, alpha=0.7)
ax.plot(observDF['Time'], model2.fittedvalues, color='red', linewidth=2, label='Regression Line')

# Add confidence interval
newx_time = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 100)
newx_time_df = pd.DataFrame({'const': 1, 'Time': newx_time})
pred_time = model2.get_prediction(newx_time_df)
pred_time_summary = pred_time.summary_frame(alpha=0.05)
ax.plot(newx_time, pred_time_summary['mean_ci_lower'], 'r--', linewidth=1, label='95% CI')
ax.plot(newx_time, pred_time_summary['mean_ci_upper'], 'r--', linewidth=1)
ax.set_xlabel('Time')
ax.set_ylabel('Anxiety')
ax.set_title('Anxiety vs Time\nwith Regression Line')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Save the plot
fig2, ax2 = plt.subplots(figsize=(8, 6))
ax2.scatter(observDF['Time'], observDF['Anxiety'], color='purple', s=80, alpha=0.7)
ax2.plot(observDF['Time'], model2.fittedvalues, color='red', linewidth=2)
ax2.plot(newx_time, pred_time_summary['mean_ci_lower'], 'r--', linewidth=1)
ax2.plot(newx_time, pred_time_summary['mean_ci_upper'], 'r--', linewidth=1)
ax2.set_xlabel('Time')
ax2.set_ylabel('Anxiety')
ax2.set_title('Bivariate Regression: Anxiety vs Time')
ax2.grid(True, alpha=0.3)
plt.savefig('anxiety_time_regression.png', dpi=100, bbox_inches='tight')
plt.close()
```

---

**SCATTER PLOT INTERPRETATION:**

The scatter plot shows a strong linear relationship between Time and Anxiety with a high R^2 value. Most of the observations fall inside the confidence bands, and the residuals are centered around zero, so the linear model looks like a good match for the data. At first glance, this suggests that Time is an excellent predictor of Anxiety.

However, upon further examination there are some potential issues. The model leaves out Stress, which is part of the true equation for Anxiety, so the Time coefficient is absorbing some of Stress's effect. Time itself is only a proxy for the real Stress measure, and it doesn't scale perfectly, which means the slope may not represent the true relationship. Even though the R² value is very high, that alone doesn't prove the coefficients are correct.

Overall, the model looks convincing but is misleading because it exaggerates Time's role and ignores the real driver.

---

## Question 5: Multiple Regression of Anxiety on StressSurvey and Time


### Multiple Regression Model

```{python}
#| echo: false
# Fit the multiple regression model: Anxiety ~ StressSurvey + Time
X3 = observDF[['StressSurvey', 'Time']]
X3_sm = sm.add_constant(X3)
model3 = sm.OLS(y, X3_sm).fit()

# Display the regression results
print(model3.summary())
```

### Estimated Coefficients for Multiple Regression

```{python}
#| echo: false
# Print the estimated coefficients for the multiple regression model
intercept_mult = model3.params['const']
stress_coef_mult = model3.params['StressSurvey']
time_coef_mult = model3.params['Time']

print("Estimated Coefficients (Anxiety ~ StressSurvey + Time):")
print("=====================================================")
print(f"Intercept (β₀): {intercept_mult:.4f}")
print(f"StressSurvey (β₁): {stress_coef_mult:.4f}")
print(f"Time (β₂): {time_coef_mult:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_mult:.4f} + {stress_coef_mult:.4f} × StressSurvey + {time_coef_mult:.4f} × Time")
```

### Scatter Plot: Multiple Regression Visualization

```{python}
#| echo: false
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Get R-squared for the plot title
r_squared_multiple = model3.rsquared

# Create a comprehensive visualization with multiple subplots
fig = plt.figure(figsize=(16, 5))

# Plot 1: 3D Scatter Plot with Regression Plane
ax1 = fig.add_subplot(131, projection='3d')

# Scatter plot of actual data
scatter = ax1.scatter(observDF['StressSurvey'], observDF['Time'], observDF['Anxiety'], 
                     c='blue', marker='o', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Create a mesh for the regression plane
stress_range = np.linspace(observDF['StressSurvey'].min(), observDF['StressSurvey'].max(), 20)
time_range = np.linspace(observDF['Time'].min(), observDF['Time'].max(), 20)
stress_mesh, time_mesh = np.meshgrid(stress_range, time_range)

# Calculate predicted values for the mesh
anxiety_mesh = intercept_mult + stress_coef_mult * stress_mesh + time_coef_mult * time_mesh

# Plot the regression plane
surf = ax1.plot_surface(stress_mesh, time_mesh, anxiety_mesh, alpha=0.3, color='red')

ax1.set_xlabel('Stress Survey', fontsize=10)
ax1.set_ylabel('Time', fontsize=10)
ax1.set_zlabel('Anxiety', fontsize=10)
ax1.set_title('3D View: Multiple Regression\nAnxiety ~ StressSurvey + Time', fontsize=11, fontweight='bold')
ax1.view_init(elev=20, azim=45)

# Plot 2: Actual vs Fitted Values
ax2 = fig.add_subplot(132)

fitted_values_mult = model3.fittedvalues
ax2.scatter(observDF['Anxiety'], fitted_values_mult, color='darkblue', s=100, alpha=0.7, edgecolors='black', linewidth=1)

# Add perfect prediction line
min_val = min(observDF['Anxiety'].min(), fitted_values_mult.min())
max_val = max(observDF['Anxiety'].max(), fitted_values_mult.max())
ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')

ax2.set_xlabel('Actual Anxiety', fontsize=10)
ax2.set_ylabel('Fitted Anxiety', fontsize=10)
ax2.set_title('Actual vs Fitted Values\n(R² = {:.4f})'.format(r_squared_multiple), fontsize=11, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Plot 3: Residuals Plot
ax3 = fig.add_subplot(133)

residuals_mult = model3.resid
ax3.scatter(fitted_values_mult, residuals_mult, color='darkgreen', s=100, alpha=0.7, edgecolors='black', linewidth=1)
ax3.axhline(y=0, color='red', linewidth=2, linestyle='--')
ax3.set_xlabel('Fitted Values', fontsize=10)
ax3.set_ylabel('Residuals', fontsize=10)
ax3.set_title('Residuals vs Fitted Values\n(Checking Assumptions)', fontsize=11, fontweight='bold')
ax3.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('multiple_regression_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n✓ Multiple regression visualization saved as 'multiple_regression_visualization.png'")
```

### Interpretation: Comparison to True Relationship

---

**INTERPRETATION:**

The multiple regression gives the equation Anxiety = 0.589 + 1.427 × StressSurvey − 2.780 × Time, with R² = 0.935. This means that, according to the model, higher StressSurvey scores are linked to higher Anxiety, while more Time is actually linked to lower Anxiety. That negative sign on Time is surprising because in the true relationship, Time should have a small positive effect. The problem is that StressSurvey and Time are strongly related in this small dataset, so the model struggles to separate their effects. This "multicollinearity" makes the coefficients unstable—StressSurvey's effect looks too large, and Time's effect even flips direction. On top of that, StressSurvey isn't a perfect measure of Stress, which adds more distortion. The high R² shows the line fits the data well, but the coefficients don't give a trustworthy picture of the true process. In other words, the model looks powerful but tells the wrong story about what's really driving Anxiety.

---


## Multiple Regression of Anxiety on Stress and Time


### Multiple Regression Model (Using Actual Stress)

```{python}
#| echo: false
# Fit the multiple regression model: Anxiety ~ Stress + Time
X4 = observDF[['Stress', 'Time']]
X4_sm = sm.add_constant(X4)
model4 = sm.OLS(y, X4_sm).fit()

# Display the regression results
print(model4.summary())
```

### Estimated Coefficients (Stress + Time Model)

```{python}
#| echo: false
# Print the estimated coefficients for Stress + Time model
intercept_stress = model4.params['const']
stress_coef_true = model4.params['Stress']
time_coef_stress = model4.params['Time']

print("Estimated Coefficients (Anxiety ~ Stress + Time):")
print("================================================")
print(f"Intercept (β₀): {intercept_stress:.4f}")
print(f"Stress (β₁): {stress_coef_true:.4f}")
print(f"Time (β₂): {time_coef_stress:.4f}")
print(f"\nRegression Equation: Anxiety = {intercept_stress:.4f} + {stress_coef_true:.4f} × Stress + {time_coef_stress:.4f} × Time")
```

### Comparison with True Relationship (Perfect Match Expected!)

---

**INTERPRETATION:**

The multiple regression of Anxiety on both Stress and Time gives us Anxiety = 0.000 + 1.000 × Stress + 0.100 × Time, with R² = 0.935. This happens to be the True coefficients given to us in the packet. This shows that with the right varaibles, multiple regression can recover the true relationship. However, the difficulty comes when we don't know the true variables, and we have to use proxy variables.

---

## Question 7: Comparing the Two Multiple Regression Models

---
Both models show really high R-squared values (.935 and 1.00), which means they explain almost all of the changes in Anxiety. But that doesn’t mean the models are actually good. In the model with StressSurvey and Time, the coefficients don’t make much sense since StressSurvey comes out way too big and Time is negative even though it should be positive. The p-values show that the results are statistically significant, but that only means there is a strong pattern in the data, not that the coefficients are actually correct. This problem happens because StressSurvey is only a proxy and also because StressSurvey and Time move together, which messes up the estimates.

The model with Stress and Time works better because the coefficients match the true equation, with Stress equal to 1.00 and Time equal to 0.10. These are also statistically significant and actually meaningful.

Overall, this shows that even if a regression has a really high R-squared and very small p-values, that doesn’t mean the model is right. If the wrong variables are used or if they overlap too much, the results can still be misleading. In real-world situations, it is important to use the right variables and not just look at the statistics when making decisions.

---

## Question 8: Reflect on Real-World Implications

---

**For the multiple regression of Anxiety on both StressSurvey and Time**, we could expect to see a headline such as:

> ### **"Time on Social Media Found to DECREASE Anxiety"**

**For the multiple regression of Anxiety on Stress and Time**, we could expect to see a headline such as:

> ### **"Time on Social Media Found to INCREASE Anxiety"**

---

**Who Believes Which Headline?**

**A Typical Parent** will likely believe the second headline, as they likely see the negative effects of time on social media on their children. Additionally, the parent will likely be looking for reasons to reduce the time their children spend on social media.

**A Social Media Executive** will likely prefer the first headline, as they can use it to argue that time on social media is a good thing, and that it leads to decreased anxiety.